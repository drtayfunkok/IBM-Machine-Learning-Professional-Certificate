{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 4, Part e: Non-Negative Matrix Factorization DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise illustrates usage of Non-negative Matrix factorization and covers techniques related to sparse matrices and some basic work with Natural Langauge Processing.  We will use NMF to look at the top words for given topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the BBC dataset. These are articles collected from 5 different topics, with the data pre-processed. \n",
    "\n",
    "These data are available in the data folder (or online [here](http://mlg.ucd.ie/files/datasets/bbc.zip)). The data consists of a few files. The steps we'll be following are:\n",
    "\n",
    "* *bbc.terms* is just a list of words \n",
    "* *bbc.docs* is a list of artcles listed by topic.\n",
    "\n",
    "At a high level, we're going to \n",
    "\n",
    "1. Turn the `bbc.mtx` file into a sparse matrix (a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html) format can be useful for matrices with many values that are 0, and save space by storing the position and values of non-zero elements).\n",
    "1. Decompose that sparse matrix using NMF.\n",
    "1. Use the resulting components of NMF to analyze the topics that result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bbc.mtx') as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9635 2225 286774\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.pop(0)\n",
    "content.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Here, we will turn this into a list of tuples representing a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html). Remember the description of the file from above:\n",
    "\n",
    "* *bbc.mtx* is a list: first column is **wordID**, second is **articleID** and the third is the number of times that word appeared in that article.\n",
    "\n",
    "So, if word 1 appears in article 3, 2 times, one element of our list will be:\n",
    "\n",
    "`(1, 3, 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1),\n",
       " (1, 7, 2),\n",
       " (1, 11, 1),\n",
       " (1, 14, 1),\n",
       " (1, 15, 2),\n",
       " (1, 19, 2),\n",
       " (1, 21, 1),\n",
       " (1, 29, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsemat = [tuple(map(int,map(float,c.split()))) for c in content]\n",
    "# Let's examine the first few elements\n",
    "sparsemat[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Sparse Matrix data for NMF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [coo matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) function to turn the sparse matrix into an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drtay\\miniconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\drtay\\miniconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\drtay\\miniconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "rows = [x[0] for x in sparsemat]\n",
    "cols = [x[1] for x in sparsemat]\n",
    "values = [x[2] for x in sparsemat]\n",
    "coo = coo_matrix((values, (rows, cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is a way of decomposing a matrix of documents and words so that one of the matrices can be interpreted as the \"loadings\" or \"weights\" of each word on a topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [the NMF documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) and the [examples of topic extraction using NMF and LDA](http://scikit-learn.org/0.18/auto_examples/applications/topics_extraction_with_nmf_lda.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Here, we will import `NMF`, define a model object with 5 components, and `fit_transform` the data created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9636, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=5, init='random', random_state=818)\n",
    "doc_topic = model.fit_transform(coo)\n",
    "\n",
    "doc_topic.shape\n",
    "# we should have 9636 observations (articles) and five latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find feature with highest value per doc\n",
    "np.argmax(doc_topic, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: \n",
    "\n",
    "Check out the `components` of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2226)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is five rows, each of which is a \"topic\" containing the weights of each word on that topic. The exercise is to _get a list of the top 10 words for each topic_. We can just store this in a list of lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Just like we read in the data above, we'll have to read in the words from the `bbc.terms` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bbc.terms') as f:\n",
    "    content = f.readlines()\n",
    "words = [c.split()[0] for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for r in model.components_:\n",
    "    a = sorted([(v,i) for i,v in enumerate(r)],reverse=True)[0:12]\n",
    "    topic_words.append([words[e[1]] for e in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bondi',\n",
       "  'stanlei',\n",
       "  'continent',\n",
       "  'mortgag',\n",
       "  'bare',\n",
       "  'least',\n",
       "  'extent',\n",
       "  '200',\n",
       "  'leav',\n",
       "  'frustrat',\n",
       "  'yuan',\n",
       "  'industri'],\n",
       " ['manipul',\n",
       "  'teenag',\n",
       "  'drawn',\n",
       "  'go',\n",
       "  'prosecutor',\n",
       "  'herbert',\n",
       "  'host',\n",
       "  'protest',\n",
       "  'hike',\n",
       "  'nation',\n",
       "  'calcul',\n",
       "  'power'],\n",
       " ['dimens',\n",
       "  'hous',\n",
       "  'march',\n",
       "  'wider',\n",
       "  'owner',\n",
       "  'intend',\n",
       "  'declin',\n",
       "  'forc',\n",
       "  'posit',\n",
       "  'founder',\n",
       "  'york',\n",
       "  'unavail'],\n",
       " ['rome',\n",
       "  'ft',\n",
       "  'regain',\n",
       "  'lawmak',\n",
       "  'outright',\n",
       "  'resum',\n",
       "  'childhood',\n",
       "  'greatest',\n",
       "  'citi',\n",
       "  'stagnat',\n",
       "  'crown',\n",
       "  'bodi'],\n",
       " ['build',\n",
       "  'empir',\n",
       "  'isol',\n",
       "  'Â£12',\n",
       "  'restructur',\n",
       "  'closer',\n",
       "  'plung',\n",
       "  'depreci',\n",
       "  'durham',\n",
       "  'race',\n",
       "  'juli',\n",
       "  'segreg']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, each set of words relates to the corresponding topic (ie the first set of words relates to topic 'Business', etc.)\n",
    "topic_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data had 5 topics, as listed in `bbc.docs` (which these topic words relate to). \n",
    "\n",
    "```\n",
    "Business\n",
    "Entertainment\n",
    "Politics\n",
    "Sport\n",
    "Tech\n",
    "```\n",
    "\n",
    "In \"real life\", we would have found a way to use these to inform the model. But for this little demo, we can just compare the recovered topics to the original ones. And they seem to match reasonably well. The order is different, which is to be expected in this kind of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/bbc.docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/bbc.docs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m d:\n\u001b[0;32m      2\u001b[0m     doc_content \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      4\u001b[0m doc_content[:\u001b[38;5;241m8\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/bbc.docs'"
     ]
    }
   ],
   "source": [
    "with open('bbc.docs') as d:\n",
    "    doc_content = d.readlines()\n",
    "    \n",
    "doc_content[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
